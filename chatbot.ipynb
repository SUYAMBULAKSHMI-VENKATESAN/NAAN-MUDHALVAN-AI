{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5dcbyduRMu3Zm7GENqXM/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SUYAMBULAKSHMI-VENKATESAN/NAAN-MUDHALVAN-AI/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWimcTeTmQY5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "with open(\"dialogs.txt\", 'r', encoding='utf-8') as file:\n",
        "    data = file.read()\n",
        "print(data)\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentences = data.split('\\n')\n",
        "\n",
        "\n",
        "print(sentences)\n",
        "\n",
        "# Preprocessing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess data\n",
        "preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
        "print(preprocessed_sentences)\n",
        "\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "lines = preprocessed_sentences\n",
        "for i in range(0, len(lines) - 1, 2):\n",
        "    input_texts.append(lines[i].strip())  # User input (input text)\n",
        "    target_texts.append(lines[i + 1].strip())\n",
        "\n",
        "\n",
        "print(input_texts)\n",
        "print(target_texts)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Convert input texts and target texts into numerical form\n",
        "X_input = vectorizer.fit_transform(input_texts)\n",
        "X_target = vectorizer.fit_transform(target_texts)\n",
        "\n",
        "print(X_input)\n",
        "print(X_target)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding, Masking\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Tokenizing input and target texts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(input_texts + target_texts)\n",
        "\n",
        "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "max_sequence_length = max(len(seq) for seq in input_sequences)  # Adjust if input and target lengths differ\n",
        "\n",
        "# Padding sequences to make them uniform in length\n",
        "padded_input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length)\n",
        "padded_target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 for 0 padding\n",
        "\n",
        "# Building the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=max_sequence_length))\n",
        "model.add(Masking(mask_value=0))  # Masking zero-padding\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model.fit(padded_input_sequences, np.expand_dims(padded_target_sequences, axis=-1), epochs=50, batch_size=32)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('chatbot_lstm_model.h5')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "# Preprocess your data if necessary\n",
        "# Initialize CountVectorizer to convert text data to vectors\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Convert input texts and target texts into numerical form\n",
        "X_input = vectorizer.fit_transform(input_texts)\n",
        "y = target_texts\n",
        "\n",
        "\n",
        "# Train a Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_input, y)\n",
        "\n",
        "# Example user input\n",
        "user_query = input()\n",
        "\n",
        "# Convert the user input into vector form\n",
        "user_query_vector = vectorizer.transform([user_query])\n",
        "\n",
        "# Predict the response using the trained classifier\n",
        "predicted_response = clf.predict(user_query_vector)\n",
        "\n",
        "print(\"User Query:\", user_query)\n",
        "print(\"Bot Response:\", predicted_response[0])\n"
      ]
    }
  ]
}